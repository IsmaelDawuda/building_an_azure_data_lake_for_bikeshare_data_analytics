{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e60c1c45-6116-4356-8df9-3a39c2c53e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5210b65-9747-47a5-aa5e-02263b5b6f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"bikeshare\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23a7f13-12ef-48a6-944f-3a0b187cfadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9e4652-ede6-451c-84ba-6aa1f3b2963f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading and Writing Data to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5c675b-e178-4df2-9c61-45f445a86f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[ spark.sql(f\"DROP TABLE IF EXISTS {table}\") for table in ['payments', 'trips', 'riders', 'stations', 'trip_dates', 'payment_dates'] ]\n",
    "\n",
    "payment_df = spark.read.format('csv').option('sep', ',').load('/FileStore/payments.csv')\n",
    "trip_df = spark.read.format('csv').option('sep', ',').load('/FileStore/trips.csv')\n",
    "rider_df = spark.read.format('csv').option('sep', ',').load('/FileStore/riders.csv')\n",
    "station_df = spark.read.format('csv').option('sep', ',').load('/FileStore/stations.csv')\n",
    "\n",
    "\n",
    "dataframes = {\n",
    "    'payments': payment_df,\n",
    "    'trips': trip_df,\n",
    "    'riders': rider_df,\n",
    "    'stations': station_df\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df.write.format('delta').mode('overwrite').saveAsTable(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f2986b-33ac-484b-9aa8-7a60e84a1ecf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "display raw data"
    }
   },
   "outputs": [],
   "source": [
    "for table_name, table in dataframes.items():\n",
    "    displayHTML(f\"<h3>{table_name}_table</h3>\")  # Display the table name as a title\n",
    "    table.show() # Display the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f82cad-3c42-4997-8640-4b1e761e6570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Formatting the columns to reflect the schema design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf46672-81c8-4e37-91e2-688295e32cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_columns(table_path: str, column_rename_dict: dict, column_type_dict: dict) -> None:\n",
    "    \"\"\"\n",
    "    Formats columns in a Spark table by renaming and casting types.\n",
    "\n",
    "    Args:\n",
    "        table_path (str): The path to the table.\n",
    "        column_rename_dict (dict): A dictionary mapping old column names to new column names.\n",
    "        column_type_dict (dict): A dictionary mapping column names to their new types.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Read the table\n",
    "    df = spark.read.table(table_path)\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.select([col(c).alias(column_rename_dict.get(c, c)) for c in df.columns])\n",
    "\n",
    "    # Cast column types\n",
    "    df = df.select([col(c).cast(column_type_dict.get(c, df.schema[c].dataType)) for c in df.columns])\n",
    "\n",
    "    # Write the transformed DataFrame back to the table\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(table_path)\n",
    "\n",
    "\n",
    "\n",
    "# Column renaming and type definitions\n",
    "columns_types = {\n",
    "    'payments': ({'_c0': 'payment_id', '_c1': 'date_id', '_c2': 'amount', '_c3': 'rider_id'}, {'payment_id': 'int', 'amount': 'decimal', 'date_id': 'date', 'rider_id': 'int'}),\n",
    "    'trips': ({'_c0': 'trip_id', '_c1': 'rideable_type', '_c2': 'started_at', '_c3': 'ended_at', '_c4': 'start_station_id', '_c5': 'end_station_id', '_c6': 'rider_id'}, {'trip_id': 'string', 'rideable_type': 'string', 'started_at': 'timestamp', 'ended_at': 'timestamp', 'start_station_id': 'string', 'end_station_id': 'string', 'rider_id': 'int'}),\n",
    "    'riders': ({'_c0': 'rider_id', '_c1': 'first', '_c2': 'last', '_c3': 'address', '_c4': 'birthday', '_c5': 'account_start_date', '_c6': 'account_end_date', '_c7': 'is_member'}, {'rider_id': 'int', 'first': 'string', 'last': 'string', 'address': 'string', 'birthday': 'date', 'account_start_date': 'date', 'account_end_date': 'date', 'is_member': 'boolean'}),\n",
    "    'stations': ({'_c0': 'station_id', '_c1': 'name', '_c2': 'latitude', '_c3': 'longitude'}, {'station_id': 'string', 'name': 'string', 'latitude': 'float', 'longitude': 'float'})\n",
    "}\n",
    "\n",
    "# Apply transformations for each table\n",
    "for table, (columns, types) in columns_types.items():\n",
    "    format_columns(table, columns, types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b4a9db-e9d2-4c39-a328-d0df0ea87098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Adding columns to address business outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a641053-ef9c-4ee0-980b-c936cbf307dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read tables\n",
    "dataframes = {\n",
    "    'trips': spark.read.table('trips'),\n",
    "    'riders': spark.read.table('riders'),\n",
    "    'payments': spark.read.table('payments')\n",
    "}\n",
    "\n",
    "# Calculate trip duration and time_id\n",
    "dataframes['trips'] = dataframes['trips'].withColumn(\"duration\", (col(\"ended_at\") - col(\"started_at\")).cast(\"long\")) \\\n",
    "                                         .withColumn(\"time_id\", date_trunc(\"hour\", col(\"started_at\")))\n",
    "\n",
    "# Calculate age at account start\n",
    "dataframes['riders'] = dataframes['riders'].withColumn(\"age_at_account_start\", (datediff(col(\"account_start_date\"), col(\"birthday\")) / 365).cast(\"int\"))\n",
    "\n",
    "# Write updated rider data\n",
    "dataframes['riders'].write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable('riders')\n",
    "\n",
    "# List rider columns excluding 'rider_id'\n",
    "rider_columns = [col for col in dataframes['riders'].columns if col != 'rider_id']\n",
    "\n",
    "# Join trip and rider data, calculate age at ride time\n",
    "dataframes['trips'] = dataframes['trips'].join(dataframes['riders'].select('rider_id', 'birthday'), on='rider_id', how='inner') \\\n",
    "                                         .withColumn(\"age_at_ride_time\", (datediff(to_date(col(\"started_at\")), col(\"birthday\")) / 365).cast(\"int\")) \\\n",
    "                                         .select('trip_id', 'duration', 'rideable_type', 'age_at_ride_time', 'started_at', 'ended_at', 'start_station_id', 'end_station_id', 'time_id', 'rider_id')\n",
    "\n",
    "# Write updated trip data\n",
    "dataframes['trips'].write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable('trips')\n",
    "\n",
    "# Write payment data\n",
    "dataframes['payments'].select('payment_id', 'amount', 'date_id', 'rider_id').write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable('payments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56943bd-0627-4dda-a953-3506e6257ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Date Dimensions\n",
    "Separate date dimension tables will be created for payment and trip data due to differences in their time granularity:\n",
    "\n",
    "The trip date dimension captures time-of-day info (morning, afternoon, evening, night) at an hourly level. The payment date dimension focuses on spending trends by month, quarter, and year at a daily level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78804d3-1284-4b6e-9049-352e14bac62c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read and cache tables\n",
    "payment_df, trip_df = (spark.read.table('payments').cache(), spark.read.table('trips').cache())\n",
    "\n",
    "# Get min and max dates for payment and trip\n",
    "payment_min_date, payment_max_date = payment_df.select(min('date_id'), max('date_id')).first()\n",
    "trip_min_date, trip_max_date = trip_df.select(min('time_id'), max('time_id')).first()\n",
    "\n",
    "# Log date ranges\n",
    "print(f\"Trip Dates: {trip_min_date} to {trip_max_date}\")\n",
    "print(f\"Payment Dates: {payment_min_date} to {payment_max_date}\")\n",
    "\n",
    "# Create date and time sequences\n",
    "sequences = [\n",
    "    spark.sql(f\"SELECT explode(sequence(to_date('{payment_min_date}'), to_date('{payment_max_date}'), INTERVAL 1 DAY)) AS date\").createOrReplaceTempView('payment_dates_view'),\n",
    "    spark.sql(f\"SELECT explode(sequence(to_timestamp('{trip_min_date}'), to_timestamp('{trip_max_date}'), INTERVAL 1 HOUR)) AS time\").createOrReplaceTempView('trip_dates_view')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4ba282-101d-49ca-bd88-f1e7ad6acca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM trip_dates_view LIMIT 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca6bb6c-1d78-4a05-a896-a3ea617bf2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trip_dates_query = f\"\"\"\n",
    "SELECT\n",
    "    time AS time_id,\n",
    "    dayofweek(time) AS day_of_week,\n",
    "    CASE \n",
    "        WHEN hour(time) BETWEEN 5 AND 11 THEN 'morning'\n",
    "        WHEN hour(time) BETWEEN 12 AND 16 THEN 'afternoon'\n",
    "        WHEN hour(time) BETWEEN 17 AND 21 THEN 'evening'\n",
    "        ELSE 'night'\n",
    "    END AS time_of_day\n",
    "FROM trip_dates_view\n",
    "ORDER BY time\n",
    "\"\"\"\n",
    "\n",
    "trip_dates = spark.sql(trip_dates_query)\n",
    "trip_dates.write.format('delta').mode('overwrite').saveAsTable('trip_dates')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d4f3a2-d4a1-40a0-a9b6-3e44a4490957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the SQL query  for payment dates\n",
    "payment_dates_query = f\"\"\"\n",
    "SELECT\n",
    "    date AS date_id,\n",
    "    month(date) AS month,\n",
    "    quarter(date) AS quarter,\n",
    "    year(date) AS year\n",
    "FROM payment_dates_view\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "\n",
    "payment_dates = spark.sql(payment_dates_query)\n",
    "payment_dates.write.format('delta').mode('overwrite').saveAsTable('payment_dates')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1492c8ae-d8dc-41e7-b185-c6a9bbbd573f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Business Questions to Address\n",
    "\n",
    "- Analyze how much time is spent per ride\n",
    "  * Based on date and time factors such as day of week and time of day\n",
    "  * Based on which station is the starting and / or ending station\n",
    "  * Based on age of the rider at time of the ride\n",
    "  * Based on whether the rider is a member or a casual rider\n",
    "- Analyze how much money is spent\n",
    "  * Per month, quarter, year\n",
    "  * Per member, based on the age of the rider at account start\n",
    "- EXTRA CREDIT - Analyze how much money is spent per member\n",
    "  * Based on how many rides the rider averages per month\n",
    "  * Based on how many minutes the rider spends on a bike per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce04ed99-88e8-4284-a07b-c179492b4612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the fact and dimension tables\n",
    "tables = ['payments', 'trips', 'riders', 'stations', 'trip_dates', 'payment_dates']\n",
    "payment_df, trip_df, rider_df, station_df, trip_date_df, payment_date_df = [spark.read.table(table) for table in tables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30fdfaa9-4829-409d-b1a9-e60f5ce3f9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trip Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111ba6e9-1cd9-4e74-a866-d0eb21d7604d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_trip_data(df: DataFrame, group_col: str, agg_func: Column, alias: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes trip data by joining with the trip date DataFrame and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing trip data.\n",
    "        group_col (str): The column to group by.\n",
    "        agg_func (Column): The aggregation function to apply (e.g., avg, sum).\n",
    "        alias (str): The alias for the aggregated column.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.join(trip_date_df, 'time_id')\\\n",
    "        .groupBy(group_col)\\\n",
    "        .agg(agg_func('duration').alias(alias))\\\n",
    "        .orderBy(alias, ascending=False)\\\n",
    "        .show()\n",
    "\n",
    "\n",
    "# Analyze how much time is spent per ride on average based on day of week\n",
    "analyze_trip_data(trip_df, 'day_of_week', avg, 'duration_in_seconds_avg')\n",
    "\n",
    "# Analyze how much time is spent per ride in total based on day of week\n",
    "analyze_trip_data(trip_df, 'day_of_week', sum, 'duration_in_seconds_sum')\n",
    "\n",
    "# Analyze how much time is spent per ride on average based on time of day\n",
    "analyze_trip_data(trip_df, 'time_of_day', avg, 'duration_in_seconds_avg')\n",
    "\n",
    "# Analyze how much time is spent per ride in total based on time of day\n",
    "analyze_trip_data(trip_df, 'time_of_day', sum, 'duration_in_seconds_sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0134da3a-dbd9-42c6-a4dd-404931621940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def analyze_duration(df: DataFrame, group_col: str, agg_func: Column, alias: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes duration data by grouping and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing duration data.\n",
    "        group_col (str): The column to group by.\n",
    "        agg_func (Column): The aggregation function to apply (e.g., avg, sum).\n",
    "        alias (str): The alias for the aggregated column.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.groupBy(group_col)\\\n",
    "      .agg(agg_func('duration').alias(alias))\\\n",
    "      .orderBy(alias, ascending=False)\\\n",
    "      .show()\n",
    "\n",
    "# Avg and total duration per ride by start station\n",
    "analyze_duration(trip_df, 'start_station_id', avg, 'duration_in_seconds_avg')\n",
    "analyze_duration(trip_df, 'start_station_id', sum, 'duration_in_seconds_sum')\n",
    "\n",
    "# Avg and total duration per ride by end station\n",
    "analyze_duration(trip_df, 'end_station_id', avg, 'duration_in_seconds_avg')\n",
    "analyze_duration(trip_df, 'end_station_id', sum, 'duration_in_seconds_sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b689e0a7-52e4-43ed-ad04-4728ffd3ed5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import Column\n",
    "\n",
    "def analyze_duration_by_age(df: DataFrame, group_col: str, agg_func: Column, alias: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes duration data by joining with the rider DataFrame, grouping by the specified column, and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing duration data.\n",
    "        group_col (str): The column to group by.\n",
    "        agg_func (Column): The aggregation function to apply (e.g., avg, sum).\n",
    "        alias (str): The alias for the aggregated column.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.join(rider_df, df.rider_id == rider_df.rider_id)\\\n",
    "      .groupBy(group_col)\\\n",
    "      .agg(agg_func('duration').alias(alias))\\\n",
    "      .orderBy(alias, ascending=False)\\\n",
    "      .show()\n",
    "\n",
    "\n",
    "# Avg and total duration by age at account start\n",
    "analyze_duration_by_age(trip_df, 'age_at_account_start', avg, 'duration_in_seconds_avg')\n",
    "analyze_duration_by_age(trip_df, 'age_at_account_start', sum, 'duration_in_seconds_sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5803bc12-edb6-47ea-a0ab-b34e54834e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_duration_by_membership(df: DataFrame, group_col: str, agg_func: Column, alias: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes duration data by joining with the rider DataFrame, grouping by membership status, and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing duration data.\n",
    "        group_col (str): The column to group by.\n",
    "        agg_func (Column): The aggregation function to apply (e.g., avg, sum).\n",
    "        alias (str): The alias for the aggregated column.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.join(rider_df, 'rider_id')\\\n",
    "      .groupBy(group_col)\\\n",
    "      .agg(agg_func('duration').alias(alias))\\\n",
    "      .orderBy(alias, ascending=False)\\\n",
    "      .show()\n",
    "\n",
    "\n",
    "# Avg and total duration by rider membership\n",
    "analyze_duration_by_membership(trip_df, 'is_member', avg, 'duration_in_seconds_avg')\n",
    "analyze_duration_by_membership(trip_df, 'is_member', sum, 'duration_in_seconds_sum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a362792-1768-4999-832d-c5a2426e01bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Payment Table Queries For Analyzing Payment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ea5766-5f3a-405c-99c4-9909ddc8d2f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_payment_data(df: DataFrame, group_col: str, agg_funcs: list, aliases: list):\n",
    "    for agg_func, alias in zip(agg_funcs, aliases):\n",
    "        df.join(payment_date_df, 'date_id')\\\n",
    "          .groupBy(group_col)\\\n",
    "          .agg(agg_func('amount').alias(alias))\\\n",
    "          .orderBy(alias, ascending=False)\\\n",
    "          .show()\n",
    "\n",
    "# Aggregation functions and their aliases\n",
    "agg_funcs = [sum, avg]\n",
    "aliases = ['amount_sum', 'amount_avg']\n",
    "\n",
    "# Analyze spending by different time periods\n",
    "for group_col in ['month', 'quarter', 'year']:\n",
    "    analyze_payment_data(payment_df, group_col, agg_funcs, aliases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544763ab-fb04-44c9-8493-9d70ca0eb828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_member_payment_data(df: DataFrame, group_col: str, agg_func: Column, alias: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes payment data for members by joining with the rider DataFrame, \n",
    "    grouping by the specified column, and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing payment data.\n",
    "        group_col (str): The column to group by.\n",
    "        agg_func (Column): The aggregation function to apply (e.g., avg, sum).\n",
    "        alias (str): The alias for the aggregated column.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.join(rider_df, 'rider_id')\\\n",
    "      .where(rider_df.is_member == True)\\\n",
    "      .groupBy(group_col)\\\n",
    "      .agg(agg_func('amount').alias(alias))\\\n",
    "      .orderBy(alias, ascending=False)\\\n",
    "      .show()\n",
    "\n",
    "# Agg functions and their aliases\n",
    "agg_funcs = [avg, sum]\n",
    "aliases = ['amount_avg', 'amount_sum']\n",
    "\n",
    "# Analyze spending by members by age at account start\n",
    "for agg_func, alias in zip(agg_funcs, aliases):\n",
    "    analyze_member_payment_data(payment_df, 'age_at_account_start', agg_func, alias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbda4068-daea-4260-a25b-fdabc782c5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extra Credit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0539bf-215a-4a7e-a872-7c79819825cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Avg spending per member by monthly ride count\n",
    "trip_df.join(payment_df, 'rider_id')\\\n",
    "    .select('rider_id', 'time_id', 'amount', 'trip_id')\\\n",
    "    .join(rider_df.where(rider_df.is_member == True), 'rider_id')\\\n",
    "    .withColumn('month', month('time_id'))\\\n",
    "    .groupby('rider_id', 'month')\\\n",
    "    .agg(avg('amount').alias('avg_amount'), count('trip_id').alias('num_rides'))\\\n",
    "    .orderBy('num_rides', ascending=False)\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb29adc-8e9a-4561-9066-6f993e58c333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Avg spending per member by monthly bike usage\n",
    "trip_df.join(rider_df, 'rider_id')\\\n",
    "    .join(payment_df, 'rider_id')\\\n",
    "    .filter(rider_df.is_member)\\\n",
    "    .withColumn('month', month('time_id'))\\\n",
    "    .withColumn('minutes', (trip_df.duration / 60).cast('int'))\\\n",
    "    .groupBy('rider_id', 'minutes', 'month')\\\n",
    "    .agg(\n",
    "        avg('amount').alias('avg_amount'),\n",
    "        avg('duration').alias('avg_duration')\n",
    "    )\\\n",
    "    .orderBy('avg_duration', ascending=False)\\\n",
    "    .show()\n",
    "\n",
    "# Investigate extended usage of a specific rider\n",
    "trip_df.filter(trip_df.rider_id == 1088)\\\n",
    "    .select('rider_id', 'started_at', 'ended_at', 'duration')\\\n",
    "    .orderBy('duration', ascending=False)\\\n",
    "    .show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 215988896508013,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "adls_bikeshare_project_IsmaelDawuda",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
