{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e60c1c45-6116-4356-8df9-3a39c2c53e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame, Column\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5210b65-9747-47a5-aa5e-02263b5b6f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"bikeshare\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9e4652-ede6-451c-84ba-6aa1f3b2963f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading and Writing Data to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5c675b-e178-4df2-9c61-45f445a86f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "[ spark.sql(f\"DROP TABLE IF EXISTS {table}\") for table in ['payments', 'trips', 'riders', 'stations', 'trip_dates', 'payment_dates'] ]\n",
    "\n",
    "payment_df = spark.read.format('csv').option('sep', ',').load('/FileStore/payments.csv')\n",
    "trip_df = spark.read.format('csv').option('sep', ',').load('/FileStore/trips.csv')\n",
    "rider_df = spark.read.format('csv').option('sep', ',').load('/FileStore/riders.csv')\n",
    "station_df = spark.read.format('csv').option('sep', ',').load('/FileStore/stations.csv')\n",
    "\n",
    "\n",
    "dataframes = {\n",
    "    'payments': payment_df,\n",
    "    'trips': trip_df,\n",
    "    'riders': rider_df,\n",
    "    'stations': station_df\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df.write.format('delta').mode('overwrite').saveAsTable(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2870b16c-5a10-4c25-90bd-57d3ee108679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###    display raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f2986b-33ac-484b-9aa8-7a60e84a1ecf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "display raw datadisplay raw data"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<h3>payments_table</h3>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<h3>payments_table</h3>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_c0</th>\n      <th>_c1</th>\n      <th>_c2</th>\n      <th>_c3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2019-05-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2019-06-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2019-07-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2019-08-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2019-09-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>2019-10-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>2019-11-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>2019-12-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>2020-01-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>2020-02-01</td>\n      <td>9.0</td>\n      <td>1000</td>\n    </tr>\n  </tbody>\n</table>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<h3>trips_table</h3>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<h3>trips_table</h3>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89E7AA6C29227EFF</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2021-02-12 16:14:56</td>\n",
       "      <td>2021-02-12 16:21:43</td>\n",
       "      <td>525</td>\n",
       "      <td>660</td>\n",
       "      <td>71934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FEFDE2603568365</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2021-02-14 17:52:38</td>\n",
       "      <td>2021-02-14 18:12:09</td>\n",
       "      <td>525</td>\n",
       "      <td>16806</td>\n",
       "      <td>47854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E6159D746B2DBB91</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2021-02-09 19:10:18</td>\n",
       "      <td>2021-02-09 19:19:10</td>\n",
       "      <td>KA1503000012</td>\n",
       "      <td>TA1305000029</td>\n",
       "      <td>70870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B32D3199F1C2E75B</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2021-02-02 17:49:41</td>\n",
       "      <td>2021-02-02 17:54:06</td>\n",
       "      <td>637</td>\n",
       "      <td>TA1305000034</td>\n",
       "      <td>58974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83E463F23575F4BF</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2021-02-23 15:07:23</td>\n",
       "      <td>2021-02-23 15:22:37</td>\n",
       "      <td>13216</td>\n",
       "      <td>TA1309000055</td>\n",
       "      <td>39608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BDAA7E3494E8D545</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2021-02-24 15:43:33</td>\n",
       "      <td>2021-02-24 15:49:05</td>\n",
       "      <td>18003</td>\n",
       "      <td>KP1705001026</td>\n",
       "      <td>36267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A772742351171257</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2021-02-01 17:47:42</td>\n",
       "      <td>2021-02-01 17:48:33</td>\n",
       "      <td>KP1705001026</td>\n",
       "      <td>KP1705001026</td>\n",
       "      <td>50104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>295476889D9B79F8</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2021-02-11 18:33:53</td>\n",
       "      <td>2021-02-11 18:35:09</td>\n",
       "      <td>18003</td>\n",
       "      <td>18003</td>\n",
       "      <td>19618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>362087194BA4CC9A</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2021-02-27 15:13:39</td>\n",
       "      <td>2021-02-27 15:36:36</td>\n",
       "      <td>KP1705001026</td>\n",
       "      <td>KP1705001026</td>\n",
       "      <td>16732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21630F715038CCB0</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2021-02-20 08:59:42</td>\n",
       "      <td>2021-02-20 09:17:04</td>\n",
       "      <td>KP1705001026</td>\n",
       "      <td>KP1705001026</td>\n",
       "      <td>57068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_c0</th>\n      <th>_c1</th>\n      <th>_c2</th>\n      <th>_c3</th>\n      <th>_c4</th>\n      <th>_c5</th>\n      <th>_c6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>89E7AA6C29227EFF</td>\n      <td>classic_bike</td>\n      <td>2021-02-12 16:14:56</td>\n      <td>2021-02-12 16:21:43</td>\n      <td>525</td>\n      <td>660</td>\n      <td>71934</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0FEFDE2603568365</td>\n      <td>classic_bike</td>\n      <td>2021-02-14 17:52:38</td>\n      <td>2021-02-14 18:12:09</td>\n      <td>525</td>\n      <td>16806</td>\n      <td>47854</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>E6159D746B2DBB91</td>\n      <td>electric_bike</td>\n      <td>2021-02-09 19:10:18</td>\n      <td>2021-02-09 19:19:10</td>\n      <td>KA1503000012</td>\n      <td>TA1305000029</td>\n      <td>70870</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B32D3199F1C2E75B</td>\n      <td>classic_bike</td>\n      <td>2021-02-02 17:49:41</td>\n      <td>2021-02-02 17:54:06</td>\n      <td>637</td>\n      <td>TA1305000034</td>\n      <td>58974</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>83E463F23575F4BF</td>\n      <td>electric_bike</td>\n      <td>2021-02-23 15:07:23</td>\n      <td>2021-02-23 15:22:37</td>\n      <td>13216</td>\n      <td>TA1309000055</td>\n      <td>39608</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BDAA7E3494E8D545</td>\n      <td>electric_bike</td>\n      <td>2021-02-24 15:43:33</td>\n      <td>2021-02-24 15:49:05</td>\n      <td>18003</td>\n      <td>KP1705001026</td>\n      <td>36267</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>A772742351171257</td>\n      <td>classic_bike</td>\n      <td>2021-02-01 17:47:42</td>\n      <td>2021-02-01 17:48:33</td>\n      <td>KP1705001026</td>\n      <td>KP1705001026</td>\n      <td>50104</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>295476889D9B79F8</td>\n      <td>classic_bike</td>\n      <td>2021-02-11 18:33:53</td>\n      <td>2021-02-11 18:35:09</td>\n      <td>18003</td>\n      <td>18003</td>\n      <td>19618</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>362087194BA4CC9A</td>\n      <td>classic_bike</td>\n      <td>2021-02-27 15:13:39</td>\n      <td>2021-02-27 15:36:36</td>\n      <td>KP1705001026</td>\n      <td>KP1705001026</td>\n      <td>16732</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>21630F715038CCB0</td>\n      <td>classic_bike</td>\n      <td>2021-02-20 08:59:42</td>\n      <td>2021-02-20 09:17:04</td>\n      <td>KP1705001026</td>\n      <td>KP1705001026</td>\n      <td>57068</td>\n    </tr>\n  </tbody>\n</table>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<h3>riders_table</h3>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<h3>riders_table</h3>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>Diana</td>\n",
       "      <td>Clark</td>\n",
       "      <td>1200 Alyssa Squares</td>\n",
       "      <td>1989-02-13</td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Smith</td>\n",
       "      <td>397 Diana Ferry</td>\n",
       "      <td>1976-08-10</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>Karen</td>\n",
       "      <td>Smith</td>\n",
       "      <td>644 Brittany Row Apt. 097</td>\n",
       "      <td>1998-08-10</td>\n",
       "      <td>2022-02-04</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>Roberts</td>\n",
       "      <td>996 Dickerson Turnpike</td>\n",
       "      <td>1999-03-29</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>Jesse</td>\n",
       "      <td>Middleton</td>\n",
       "      <td>7009 Nathan Expressway</td>\n",
       "      <td>1969-04-11</td>\n",
       "      <td>2019-09-14</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1005</td>\n",
       "      <td>Christine</td>\n",
       "      <td>Rodriguez</td>\n",
       "      <td>224 Washington Mills Apt. 467</td>\n",
       "      <td>1974-08-27</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1006</td>\n",
       "      <td>Alicia</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>1137 Angela Locks</td>\n",
       "      <td>2004-01-30</td>\n",
       "      <td>2020-11-27</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1007</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>Fernandez</td>\n",
       "      <td>979 Phillips Ways</td>\n",
       "      <td>1988-01-11</td>\n",
       "      <td>2016-12-11</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1008</td>\n",
       "      <td>John</td>\n",
       "      <td>Crawford</td>\n",
       "      <td>7691 Evans Court</td>\n",
       "      <td>1987-02-21</td>\n",
       "      <td>2021-03-28</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1009</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Ritter</td>\n",
       "      <td>9922 Jim Crest Apt. 319</td>\n",
       "      <td>1981-02-07</td>\n",
       "      <td>2020-06-12</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_c0</th>\n      <th>_c1</th>\n      <th>_c2</th>\n      <th>_c3</th>\n      <th>_c4</th>\n      <th>_c5</th>\n      <th>_c6</th>\n      <th>_c7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000</td>\n      <td>Diana</td>\n      <td>Clark</td>\n      <td>1200 Alyssa Squares</td>\n      <td>1989-02-13</td>\n      <td>2019-04-23</td>\n      <td>None</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001</td>\n      <td>Jennifer</td>\n      <td>Smith</td>\n      <td>397 Diana Ferry</td>\n      <td>1976-08-10</td>\n      <td>2019-11-01</td>\n      <td>2020-09-01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1002</td>\n      <td>Karen</td>\n      <td>Smith</td>\n      <td>644 Brittany Row Apt. 097</td>\n      <td>1998-08-10</td>\n      <td>2022-02-04</td>\n      <td>None</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1003</td>\n      <td>Bryan</td>\n      <td>Roberts</td>\n      <td>996 Dickerson Turnpike</td>\n      <td>1999-03-29</td>\n      <td>2019-08-26</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1004</td>\n      <td>Jesse</td>\n      <td>Middleton</td>\n      <td>7009 Nathan Expressway</td>\n      <td>1969-04-11</td>\n      <td>2019-09-14</td>\n      <td>None</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1005</td>\n      <td>Christine</td>\n      <td>Rodriguez</td>\n      <td>224 Washington Mills Apt. 467</td>\n      <td>1974-08-27</td>\n      <td>2020-03-24</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1006</td>\n      <td>Alicia</td>\n      <td>Taylor</td>\n      <td>1137 Angela Locks</td>\n      <td>2004-01-30</td>\n      <td>2020-11-27</td>\n      <td>2021-12-01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1007</td>\n      <td>Benjamin</td>\n      <td>Fernandez</td>\n      <td>979 Phillips Ways</td>\n      <td>1988-01-11</td>\n      <td>2016-12-11</td>\n      <td>None</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1008</td>\n      <td>John</td>\n      <td>Crawford</td>\n      <td>7691 Evans Court</td>\n      <td>1987-02-21</td>\n      <td>2021-03-28</td>\n      <td>2021-07-01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1009</td>\n      <td>Victoria</td>\n      <td>Ritter</td>\n      <td>9922 Jim Crest Apt. 319</td>\n      <td>1981-02-07</td>\n      <td>2020-06-12</td>\n      <td>2021-11-01</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<h3>stations_table</h3>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<h3>stations_table</h3>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>Glenwood Ave &amp; Touhy Ave</td>\n",
       "      <td>42.012701</td>\n",
       "      <td>-87.66605799999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KA1503000012</td>\n",
       "      <td>Clark St &amp; Lake St</td>\n",
       "      <td>41.88579466666667</td>\n",
       "      <td>-87.63110066666668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>637</td>\n",
       "      <td>Wood St &amp; Chicago Ave</td>\n",
       "      <td>41.895634</td>\n",
       "      <td>-87.672069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13216</td>\n",
       "      <td>State St &amp; 33rd St</td>\n",
       "      <td>41.8347335</td>\n",
       "      <td>-87.6258275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18003</td>\n",
       "      <td>Fairbanks St &amp; Superior St</td>\n",
       "      <td>41.89580766666667</td>\n",
       "      <td>-87.62025316666669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KP1705001026</td>\n",
       "      <td>LaSalle Dr &amp; Huron St</td>\n",
       "      <td>41.894877</td>\n",
       "      <td>-87.632326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13253</td>\n",
       "      <td>Lincoln Ave &amp; Waveland Ave</td>\n",
       "      <td>41.948797</td>\n",
       "      <td>-87.675278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KA1503000044</td>\n",
       "      <td>Rush St &amp; Hubbard St</td>\n",
       "      <td>41.890173</td>\n",
       "      <td>-87.62618499999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KA1504000140</td>\n",
       "      <td>Winchester Ave &amp; Elston Ave</td>\n",
       "      <td>41.92403733333333</td>\n",
       "      <td>-87.67641483333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TA1305000032</td>\n",
       "      <td>Clinton St &amp; Madison St</td>\n",
       "      <td>41.882242</td>\n",
       "      <td>-87.64106600000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_c0</th>\n      <th>_c1</th>\n      <th>_c2</th>\n      <th>_c3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>525</td>\n      <td>Glenwood Ave &amp; Touhy Ave</td>\n      <td>42.012701</td>\n      <td>-87.66605799999999</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>KA1503000012</td>\n      <td>Clark St &amp; Lake St</td>\n      <td>41.88579466666667</td>\n      <td>-87.63110066666668</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>637</td>\n      <td>Wood St &amp; Chicago Ave</td>\n      <td>41.895634</td>\n      <td>-87.672069</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13216</td>\n      <td>State St &amp; 33rd St</td>\n      <td>41.8347335</td>\n      <td>-87.6258275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18003</td>\n      <td>Fairbanks St &amp; Superior St</td>\n      <td>41.89580766666667</td>\n      <td>-87.62025316666669</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>KP1705001026</td>\n      <td>LaSalle Dr &amp; Huron St</td>\n      <td>41.894877</td>\n      <td>-87.632326</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>13253</td>\n      <td>Lincoln Ave &amp; Waveland Ave</td>\n      <td>41.948797</td>\n      <td>-87.675278</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>KA1503000044</td>\n      <td>Rush St &amp; Hubbard St</td>\n      <td>41.890173</td>\n      <td>-87.62618499999999</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>KA1504000140</td>\n      <td>Winchester Ave &amp; Elston Ave</td>\n      <td>41.92403733333333</td>\n      <td>-87.67641483333334</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>TA1305000032</td>\n      <td>Clinton St &amp; Madison St</td>\n      <td>41.882242</td>\n      <td>-87.64106600000001</td>\n    </tr>\n  </tbody>\n</table>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display only the first 10 records for each raw tables\n",
    "for table_name, table in dataframes.items():\n",
    "    displayHTML(f\"<h3>{table_name}_table</h3>\")  # Display the table name as a title\n",
    "    # Convert list to DataFrame\n",
    "    df = table.toPandas()\n",
    "    \n",
    "    # Display the first 10 rows\n",
    "    displayHTML(df.head(10).to_html())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f82cad-3c42-4997-8640-4b1e761e6570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Formatting the columns to reflect the schema design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf46672-81c8-4e37-91e2-688295e32cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<h3>payments_table</h3>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<h3>payments_table</h3>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-254777733165781&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     45</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     46</span>     <span class=\"ansi-red-fg\"># Display only the first 10 records of the transformed DataFrame</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 47</span><span class=\"ansi-red-fg\">     </span>display<span class=\"ansi-blue-fg\">(</span>transformed_df<span class=\"ansi-blue-fg\">.</span>head<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">head</span><span class=\"ansi-blue-fg\">(self, n)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1744</span>             rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>head<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1745</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-green-fg\">if</span> rs <span class=\"ansi-green-fg\">else</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1746</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1747</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1748</span>     <span class=\"ansi-green-fg\">def</span> first<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    767</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">5</span><span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;Bob&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    768</span>         &#34;&#34;&#34;\n",
       "<span class=\"ansi-green-fg\">--&gt; 769</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>limit<span class=\"ansi-blue-fg\">(</span>num<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    770</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    771</span>     <span class=\"ansi-green-fg\">def</span> tail<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> num<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    713</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-DF-ACL clusters:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    714</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 715</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>collectToPython<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    716</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> BatchedSerializer<span class=\"ansi-blue-fg\">(</span>PickleSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    717</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>: The schema of your Delta table has changed in an incompatible way since your DataFrame or\n",
       "DeltaTable object was created. Please redefine your DataFrame or DeltaTable object.\n",
       "Changes:\n",
       "Latest schema is missing field(s): _c0, _c1, _c2, _c3\n",
       "Latest schema has additional field(s): payment_id, date_id, amount, rider_id\n",
       "This check can be turned off by setting the session configuration key\n",
       "spark.databricks.delta.checkLatestSchemaOnRead to false.</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-254777733165781&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     45</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     46</span>     <span class=\"ansi-red-fg\"># Display only the first 10 records of the transformed DataFrame</span>\n<span class=\"ansi-green-fg\">---&gt; 47</span><span class=\"ansi-red-fg\">     </span>display<span class=\"ansi-blue-fg\">(</span>transformed_df<span class=\"ansi-blue-fg\">.</span>head<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">head</span><span class=\"ansi-blue-fg\">(self, n)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1744</span>             rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>head<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1745</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-green-fg\">if</span> rs <span class=\"ansi-green-fg\">else</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">-&gt; 1746</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1747</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1748</span>     <span class=\"ansi-green-fg\">def</span> first<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    767</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">5</span><span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;Bob&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    768</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 769</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>limit<span class=\"ansi-blue-fg\">(</span>num<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    770</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    771</span>     <span class=\"ansi-green-fg\">def</span> tail<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> num<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    713</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-DF-ACL clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    714</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 715</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>collectToPython<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    716</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> BatchedSerializer<span class=\"ansi-blue-fg\">(</span>PickleSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    717</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: The schema of your Delta table has changed in an incompatible way since your DataFrame or\nDeltaTable object was created. Please redefine your DataFrame or DeltaTable object.\nChanges:\nLatest schema is missing field(s): _c0, _c1, _c2, _c3\nLatest schema has additional field(s): payment_id, date_id, amount, rider_id\nThis check can be turned off by setting the session configuration key\nspark.databricks.delta.checkLatestSchemaOnRead to false.</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">AnalysisException</span>: The schema of your Delta table has changed in an incompatible way since your DataFrame or",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_columns(table_path: str, column_rename_dict: dict, column_type_dict: dict):\n",
    "    \"\"\"\n",
    "    Formats columns in a Spark table by renaming and casting types.\n",
    "\n",
    "    Args:\n",
    "        table_path (str): The path to the table.\n",
    "        column_rename_dict (dict): A dictionary mapping old column names to new column names.\n",
    "        column_type_dict (dict): A dictionary mapping column names to their new types.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the table\n",
    "    df = spark.read.table(table_path)\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.select([col(c).alias(column_rename_dict.get(c, c)) for c in df.columns])\n",
    "\n",
    "    # Cast column types\n",
    "    df = df.select([col(c).cast(column_type_dict.get(c, df.schema[c].dataType)) for c in df.columns])\n",
    "\n",
    "    # Write the transformed DataFrame back to the table\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(table_path)\n",
    "    \n",
    "    # Return the transformed DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Column renaming and type definitions\n",
    "columns_types = {\n",
    "    'payments': ({'_c0': 'payment_id', '_c1': 'date_id', '_c2': 'amount', '_c3': 'rider_id'}, {'payment_id': 'int', 'amount': 'decimal', 'date_id': 'date', 'rider_id': 'int'}),\n",
    "    'trips': ({'_c0': 'trip_id', '_c1': 'rideable_type', '_c2': 'started_at', '_c3': 'ended_at', '_c4': 'start_station_id', '_c5': 'end_station_id', '_c6': 'rider_id'}, {'trip_id': 'string', 'rideable_type': 'string', 'started_at': 'timestamp', 'ended_at': 'timestamp', 'start_station_id': 'string', 'end_station_id': 'string', 'rider_id': 'int'}),\n",
    "    'riders': ({'_c0': 'rider_id', '_c1': 'first', '_c2': 'last', '_c3': 'address', '_c4': 'birthday', '_c5': 'account_start_date', '_c6': 'account_end_date', '_c7': 'is_member'}, {'rider_id': 'int', 'first': 'string', 'last': 'string', 'address': 'string', 'birthday': 'date', 'account_start_date': 'date', 'account_end_date': 'date', 'is_member': 'boolean'}),\n",
    "    'stations': ({'_c0': 'station_id', '_c1': 'name', '_c2': 'latitude', '_c3': 'longitude'}, {'station_id': 'string', 'name': 'string', 'latitude': 'float', 'longitude': 'float'})\n",
    "}\n",
    "\n",
    "# Apply transformations for each table and display results\n",
    "for table_name, (columns, types) in columns_types.items():\n",
    "    # Apply transformations and get the transformed DataFrame\n",
    "    transformed_df = format_columns(table_name, columns, types)\n",
    "    \n",
    "    # Display the table name as a title\n",
    "    displayHTML(f\"<h3>{table_name}_table</h3>\")\n",
    "    \n",
    "    # Display only the first 10 records of the transformed DataFrame\n",
    "    display(transformed_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b4a9db-e9d2-4c39-a328-d0df0ea87098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Adding columns to address business outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a641053-ef9c-4ee0-980b-c936cbf307dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read tables\n",
    "dataframes = {\n",
    "    'trips': spark.read.table('trips'),\n",
    "    'riders': spark.read.table('riders'),\n",
    "    'payments': spark.read.table('payments')\n",
    "}\n",
    "\n",
    "# Calculate trip duration and time_id\n",
    "dataframes['trips'] = dataframes['trips'].withColumn(\"duration\", (col(\"ended_at\") - col(\"started_at\")).cast(\"long\")) \\\n",
    "                                         .withColumn(\"time_id\", date_trunc(\"hour\", col(\"started_at\")))\n",
    "\n",
    "# Calculate age at account start\n",
    "dataframes['riders'] = dataframes['riders'].withColumn(\"age_at_account_start\", (datediff(col(\"account_start_date\"), col(\"birthday\")) / 365).cast(\"int\"))\n",
    "\n",
    "# Write updated rider data\n",
    "dataframes['riders'].write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable('riders')\n",
    "\n",
    "# List rider columns excluding 'rider_id'\n",
    "rider_columns = [col for col in dataframes['riders'].columns if col != 'rider_id']\n",
    "\n",
    "# Join trip and rider data, calculate age at ride time\n",
    "dataframes['trips'] = dataframes['trips'].join(dataframes['riders'].select('rider_id', 'birthday'), on='rider_id', how='inner') \\\n",
    "                                         .withColumn(\"age_at_ride_time\", (datediff(to_date(col(\"started_at\")), col(\"birthday\")) / 365).cast(\"int\")) \\\n",
    "                                         .select('trip_id', 'duration', 'rideable_type', 'age_at_ride_time', 'started_at', 'ended_at', 'start_station_id', 'end_station_id', 'time_id', 'rider_id')\n",
    "\n",
    "# Write updated trip data\n",
    "dataframes['trips'].write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable('trips')\n",
    "\n",
    "# Write payment data\n",
    "dataframes['payments'].select('payment_id', 'amount', 'date_id', 'rider_id').write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable('payments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56943bd-0627-4dda-a953-3506e6257ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Date Dimensions\n",
    "Separate date dimension tables will be created for payment and trip data due to differences in their time granularity:\n",
    "\n",
    "The trip date dimension captures time-of-day info (morning, afternoon, evening, night) at an hourly level. The payment date dimension focuses on spending trends by month, quarter, and year at a daily level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78804d3-1284-4b6e-9049-352e14bac62c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read and cache tables\n",
    "payment_df, trip_df = (spark.read.table('payments').cache(), spark.read.table('trips').cache())\n",
    "\n",
    "# Get min and max dates for payment and trip\n",
    "payment_min_date, payment_max_date = payment_df.select(min('date_id'), max('date_id')).first()\n",
    "trip_min_date, trip_max_date = trip_df.select(min('time_id'), max('time_id')).first()\n",
    "\n",
    "# Log date ranges\n",
    "print(f\"Trip Dates: {trip_min_date} to {trip_max_date}\")\n",
    "print(f\"Payment Dates: {payment_min_date} to {payment_max_date}\")\n",
    "\n",
    "# Create date and time sequences\n",
    "sequences = [\n",
    "    spark.sql(f\"SELECT explode(sequence(to_date('{payment_min_date}'), to_date('{payment_max_date}'), INTERVAL 1 DAY)) AS date\").createOrReplaceTempView('payment_dates_view'),\n",
    "    spark.sql(f\"SELECT explode(sequence(to_timestamp('{trip_min_date}'), to_timestamp('{trip_max_date}'), INTERVAL 1 HOUR)) AS time\").createOrReplaceTempView('trip_dates_view')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65517d4-f157-4c67-9f13-fd9241647fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Payment Dates View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4ba282-101d-49ca-bd88-f1e7ad6acca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql SELECT * FROM payment_dates_view LIMIT 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513ac8ec-cbf2-4436-bf2a-9f0873b13418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Trip Dates View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a57cfe87-eee1-4549-92fc-decf3ea0e144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql SELECT * FROM trip_dates_view LIMIT 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca6bb6c-1d78-4a05-a896-a3ea617bf2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trip_dates_query = f\"\"\"\n",
    "SELECT\n",
    "    time AS time_id,\n",
    "    dayofweek(time) AS day_of_week,\n",
    "    CASE \n",
    "        WHEN hour(time) BETWEEN 5 AND 11 THEN 'morning'\n",
    "        WHEN hour(time) BETWEEN 12 AND 16 THEN 'afternoon'\n",
    "        WHEN hour(time) BETWEEN 17 AND 21 THEN 'evening'\n",
    "        ELSE 'night'\n",
    "    END AS time_of_day\n",
    "FROM trip_dates_view\n",
    "ORDER BY time\n",
    "\"\"\"\n",
    "\n",
    "trip_dates = spark.sql(trip_dates_query)\n",
    "trip_dates.write.format('delta').mode('overwrite').saveAsTable('trip_dates')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d4f3a2-d4a1-40a0-a9b6-3e44a4490957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the SQL query  for payment dates\n",
    "payment_dates_query = f\"\"\"\n",
    "SELECT\n",
    "    date AS date_id,\n",
    "    month(date) AS month,\n",
    "    quarter(date) AS quarter,\n",
    "    year(date) AS year\n",
    "FROM payment_dates_view\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "\n",
    "payment_dates = spark.sql(payment_dates_query)\n",
    "payment_dates.write.format('delta').mode('overwrite').saveAsTable('payment_dates')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1492c8ae-d8dc-41e7-b185-c6a9bbbd573f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Business Questions to Address\n",
    "\n",
    "- Analyze how much time is spent per ride\n",
    "  * Based on date and time factors such as day of week and time of day\n",
    "  * Based on which station is the starting and / or ending station\n",
    "  * Based on age of the rider at time of the ride\n",
    "  * Based on whether the rider is a member or a casual rider\n",
    "- Analyze how much money is spent\n",
    "  * Per month, quarter, year\n",
    "  * Per member, based on the age of the rider at account start\n",
    "- EXTRA CREDIT - Analyze how much money is spent per member\n",
    "  * Based on how many rides the rider averages per month\n",
    "  * Based on how many minutes the rider spends on a bike per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce04ed99-88e8-4284-a07b-c179492b4612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the fact and dimension tables\n",
    "tables = ['payments', 'trips', 'riders', 'stations', 'trip_dates', 'payment_dates']\n",
    "payment_df, trip_df, rider_df, station_df, trip_date_df, payment_date_df = [spark.read.table(table) for table in tables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30fdfaa9-4829-409d-b1a9-e60f5ce3f9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trip Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111ba6e9-1cd9-4e74-a866-d0eb21d7604d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_trip_data(df: DataFrame, group_col: str, agg_func: Column, alias: str, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyze trip data with aggregation and display results with a title.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Trip data.\n",
    "        group_col (str): Column to group by.\n",
    "        agg_func (Column): Aggregation function (avg, sum).\n",
    "        alias (str): Alias for the aggregated column.\n",
    "        title (str): Title for the results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    result_df = df.join(trip_date_df, 'time_id')\\\n",
    "                  .groupBy(group_col)\\\n",
    "                  .agg(agg_func('duration').alias(alias))\\\n",
    "                  .orderBy(alias, ascending=False)\n",
    "    \n",
    "    # Display the title and results\n",
    "    displayHTML(f\"<h3>{title}</h3>\")\n",
    "    result_df.show()\n",
    "\n",
    "# Analysis examples with titles\n",
    "analyze_trip_data(trip_df, 'day_of_week', avg, 'duration_in_seconds_avg', 'Avg Duration per Ride by Day of Week')\n",
    "analyze_trip_data(trip_df, 'day_of_week', sum, 'duration_in_seconds_sum', 'Total Duration per Ride by Day of Week')\n",
    "analyze_trip_data(trip_df, 'time_of_day', avg, 'duration_in_seconds_avg', 'Avg Duration per Ride by Time of Day')\n",
    "analyze_trip_data(trip_df, 'time_of_day', sum, 'duration_in_seconds_sum', 'Total Duration per Ride by Time of Day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0134da3a-dbd9-42c6-a4dd-404931621940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_duration(df: DataFrame, group_col: str, agg_func: Column, alias: str, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyze duration data by grouping and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing duration data.\n",
    "        group_col (str): Column to group by.\n",
    "        agg_func (Column): Aggregation function (avg, sum).\n",
    "        alias (str): Alias for the aggregated column.\n",
    "        title (str): Title for the results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    result_df = df.groupBy(group_col)\\\n",
    "                  .agg(agg_func('duration').alias(alias))\\\n",
    "                  .orderBy(alias, ascending=False)\n",
    "    \n",
    "    # Display title and results\n",
    "    displayHTML(f\"<h3>{title}</h3>\")\n",
    "    result_df.show()\n",
    "\n",
    "# Avg and total duration per ride by start station\n",
    "analyze_duration(trip_df, 'start_station_id', avg, 'duration_in_seconds_avg', 'Avg Duration per Ride by Start Station')\n",
    "analyze_duration(trip_df, 'start_station_id', sum, 'duration_in_seconds_sum', 'Total Duration per Ride by Start Station')\n",
    "\n",
    "# Avg and total duration per ride by end station\n",
    "analyze_duration(trip_df, 'end_station_id', avg, 'duration_in_seconds_avg', 'Avg Duration per Ride by End Station')\n",
    "analyze_duration(trip_df, 'end_station_id', sum, 'duration_in_seconds_sum', 'Total Duration per Ride by End Station')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b689e0a7-52e4-43ed-ad04-4728ffd3ed5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_duration_by_age(df: DataFrame, group_col: str, agg_func: Column, alias: str, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyze duration data by joining with the rider DataFrame, grouping by the specified column, and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing duration data.\n",
    "        group_col (str): Column to group by.\n",
    "        agg_func (Column): Aggregation function (avg, sum).\n",
    "        alias (str): Alias for the aggregated column.\n",
    "        title (str): Title for the results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    result_df = df.join(rider_df, df.rider_id == rider_df.rider_id)\\\n",
    "                  .groupBy(group_col)\\\n",
    "                  .agg(agg_func('duration').alias(alias))\\\n",
    "                  .orderBy(alias, ascending=False)\n",
    "    \n",
    "    # Display title and results\n",
    "    displayHTML(f\"<h3>{title}</h3>\")\n",
    "    result_df.show()\n",
    "\n",
    "# Avg and total duration by age at account start\n",
    "analyze_duration_by_age(trip_df, 'age_at_account_start', avg, 'duration_in_seconds_avg', 'Avg Duration by Age at Account Start')\n",
    "analyze_duration_by_age(trip_df, 'age_at_account_start', sum, 'duration_in_seconds_sum', 'Total Duration by Age at Account Start')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5803bc12-edb6-47ea-a0ab-b34e54834e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_duration_by_membership(df: DataFrame, group_col: str, agg_func: Column, alias: str, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyze duration data by joining with the rider DataFrame, grouping by membership status, and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing duration data.\n",
    "        group_col (str): Column to group by.\n",
    "        agg_func (Column): Aggregation function (avg, sum).\n",
    "        alias (str): Alias for the aggregated column.\n",
    "        title (str): Title for the results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    result_df = df.join(rider_df, 'rider_id')\\\n",
    "                  .groupBy(group_col)\\\n",
    "                  .agg(agg_func('duration').alias(alias))\\\n",
    "                  .orderBy(alias, ascending=False)\n",
    "    \n",
    "    # Display title and results\n",
    "    displayHTML(f\"<h3>{title}</h3>\")\n",
    "    result_df.show()\n",
    "\n",
    "# Avg and total duration by rider membership\n",
    "analyze_duration_by_membership(trip_df, 'is_member', avg, 'duration_in_seconds_avg', 'Avg Duration by Rider Membership')\n",
    "analyze_duration_by_membership(trip_df, 'is_member', sum, 'duration_in_seconds_sum', 'Total Duration by Rider Membership')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a362792-1768-4999-832d-c5a2426e01bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Payment Table Queries For Analyzing Payment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ea5766-5f3a-405c-99c4-9909ddc8d2f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_payment_data(df: DataFrame, group_col: str, agg_funcs: list, aliases: list, titles: list):\n",
    "    \"\"\"\n",
    "    Analyze payment data by joining with the payment date DataFrame, grouping by the specified column, and applying aggregation functions.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing payment data.\n",
    "        group_col (str): Column to group by.\n",
    "        agg_funcs (list): List of aggregation functions (e.g., sum, avg).\n",
    "        aliases (list): List of aliases for the aggregated columns.\n",
    "        titles (list): List of titles for the results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for agg_func, alias, title in zip(agg_funcs, aliases, titles):\n",
    "        result_df = df.join(payment_date_df, 'date_id')\\\n",
    "                      .groupBy(group_col)\\\n",
    "                      .agg(agg_func('amount').alias(alias))\\\n",
    "                      .orderBy(alias, ascending=False)\n",
    "        \n",
    "        # Display title and results\n",
    "        displayHTML(f\"<h3>{title}</h3>\")\n",
    "        result_df.show()\n",
    "\n",
    "# Aggregation functions, their aliases, and titles\n",
    "agg_funcs = [sum, avg]\n",
    "aliases = ['amount_sum', 'amount_avg']\n",
    "titles = ['Total Amount', 'Average Amount']\n",
    "\n",
    "# Analyze spending by different time periods\n",
    "for group_col in ['month', 'quarter', 'year']:\n",
    "    analyze_payment_data(payment_df, group_col, agg_funcs, aliases, [f'Total Amount by {group_col.capitalize()}', f'Average Amount by {group_col.capitalize()}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544763ab-fb04-44c9-8493-9d70ca0eb828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_member_payment_data(df: DataFrame, group_col: str, agg_func: Column, alias: str, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyze payment data for members by joining with the rider DataFrame, \n",
    "    grouping by the specified column, and applying an aggregation function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing payment data.\n",
    "        group_col (str): Column to group by.\n",
    "        agg_func (Column): Aggregation function (avg, sum).\n",
    "        alias (str): Alias for the aggregated column.\n",
    "        title (str): Title for the results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    result_df = df.join(rider_df, 'rider_id')\\\n",
    "                  .where(rider_df.is_member == True)\\\n",
    "                  .groupBy(group_col)\\\n",
    "                  .agg(agg_func('amount').alias(alias))\\\n",
    "                  .orderBy(alias, ascending=False)\n",
    "    \n",
    "    # Display title and results\n",
    "    displayHTML(f\"<h3>{title}</h3>\")\n",
    "    result_df.show()\n",
    "\n",
    "# Aggregation functions and their aliases\n",
    "agg_funcs = [avg, sum]\n",
    "aliases = ['amount_avg', 'amount_sum']\n",
    "\n",
    "# Analyze spending by members by age at account start\n",
    "titles = ['Average Amount by Age at Account Start (Members)', 'Total Amount by Age at Account Start (Members)']\n",
    "for agg_func, alias, title in zip(agg_funcs, aliases, titles):\n",
    "    analyze_member_payment_data(payment_df, 'age_at_account_start', agg_func, alias, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbda4068-daea-4260-a25b-fdabc782c5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extra Credit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0539bf-215a-4a7e-a872-7c79819825cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Avg spending per member by monthly ride count\n",
    "result_df = trip_df.join(payment_df, 'rider_id')\\\n",
    "    .select('rider_id', 'time_id', 'amount', 'trip_id')\\\n",
    "    .join(rider_df.where(rider_df.is_member == True), 'rider_id')\\\n",
    "    .withColumn('month', month('time_id'))\\\n",
    "    .groupBy('rider_id', 'month')\\\n",
    "    .agg(avg('amount').alias('avg_amount'), count('trip_id').alias('num_rides'))\\\n",
    "    .orderBy('num_rides', ascending=False)\n",
    "\n",
    "# Display title and results\n",
    "displayHTML(\"<h3>Average Spending per Member by Monthly Ride Count</h3>\")\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb29adc-8e9a-4561-9066-6f993e58c333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Avg spending per member by monthly bike usage\n",
    "result_df = trip_df.join(rider_df, 'rider_id')\\\n",
    "    .join(payment_df, 'rider_id')\\\n",
    "    .filter(rider_df.is_member)\\\n",
    "    .withColumn('month', month('time_id'))\\\n",
    "    .withColumn('minutes', (trip_df.duration / 60).cast('int'))\\\n",
    "    .groupBy('rider_id', 'minutes', 'month')\\\n",
    "    .agg(\n",
    "        avg('amount').alias('avg_amount'),\n",
    "        avg('duration').alias('avg_duration')\n",
    "    )\\\n",
    "    .orderBy('avg_duration', ascending=False)\n",
    "\n",
    "# Display title and results\n",
    "displayHTML(\"<h3>Average Spending per Member by Monthly Bike Usage</h3>\")\n",
    "result_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 254777733165802,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "adls_bikeshare_project_IsmaelDawuda",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
